{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AIRBNB](https://www.stevenridercpa.au/wp-content/uploads/2022/09/airbnb-tax.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb - Price Prediction\n",
    "-------\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](##Introduction)\n",
    "    - [Problem Statement](###Problem-Statement)\n",
    "    - [Objective](###Objective)\n",
    "    - [Dataset Overview](###Dataset-Overview)\n",
    "2. [Setup](##Setup)\n",
    "2. [Data Loading and Exploration](##Data-Loading-and-Exploration)\n",
    "    - [Loading the Dataset](###Loading-the-Dataset)\n",
    "    - [Exploratory Data Analysis (EDA)](###Exploratory-Data-Analysis-(EDA))\n",
    "    - [Data Preprocessing](###Data-Preprocessing)\n",
    "3. [Baseline Model](##Baseline-Model)\n",
    "    - [Preparing the Inputs](###Preparing-the-Inputs)\n",
    "    - [Model Architecture](###Model-Architecture)\n",
    "    - [Model Compilation](###Model-Compilation)\n",
    "    - [Model Training](###Model-Training)\n",
    "4. [Hyperparameter Tuning](##Hyperparameter-Tuning)\n",
    "    - [Grid Search Setup](###Grid-Search-Setup)\n",
    "    - [Execution of Grid Search](###Execution-of-Grid-Search)\n",
    "    - [Analysis of Grid Search Results](###Analysis-of-Grid-Search-Results)\n",
    "5. [Participation in Kaggle Competition](##Participation-in-Kaggle-Competition)\n",
    "    - [Submission Preparation](###Submission-Preparation)\n",
    "    - [Submission to Kaggle](###Submission-to-Kaggle)\n",
    "6. [Conclusion](##Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "A dataset containing information of accommodations published in AirBnB with their respective prices is presented. The size of the train dataset is approximately 1.5 Gb, and 0.5 Gb for the test dataset. This has 84 predictor variables that can be used as they see fit.\n",
    "\n",
    "The objective is to assign the correct price to the listed accommodations. \n",
    "\n",
    "In addition to the dataset, you are provided with this notebook containing the data loading script and a baseline model corresponding to a feed forward architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The primary objective of this project is to develop a predictive model capable of accurately estimating the rental prices of accommodations listed on AirBnB. By leveraging a dataset consisting of various attributes and historical pricing data of listed accommodations, we aim to build a model that minimizes the error in price prediction. \n",
    "\n",
    "The success of this endeavor will be evaluated based on the Mean Absolute Error (MAE) metric, with the goal of achieving an MAE of less than 70 points (based on the [participation in Kaggle](##Participation-in-Kaggle-Competition)). This objective aligns with the criteria set forth in the associated Kaggle competition, which serves as a structured platform for benchmarking the performance of our model against others.\n",
    "\n",
    "Several tasks have been outlined to aid in the accomplishment of this objective:\n",
    "\n",
    "- Conduct thorough [exploratory data analysis](###Exploratory-Data-Analysis-(EDA)) to understand the underlying patterns and characteristics of the data.\n",
    "- [Preprocess the data](###Data-Preprocessing) to ensure it is well-suited for training machine learning models.\n",
    "- Establish a [baseline model](##Baseline-Model) using a feed-forward neural network architecture, against which further models and techniques can be compared.\n",
    "- Engage in a methodical [grid search](###Grid-Search-Setup) to identify the optimal hyperparameters for our model, utilizing tools such as [Weights and Biases](https://wandb.ai/site) for systematic exploration and logging.\n",
    "- Incorporate advanced techniques including [Batch Normalization](###Batch-Normalization) and [Gradient Normalization/Clipping](###Gradient-Normalization-and/or-Gradient-Clipping) to enhance the learning process and stability of the model.\n",
    "- Continuously evaluate the performance of the model, iterating on the architecture and training process as necessary to inch closer to the desired MAE goal.\n",
    "\n",
    "By adhering to a structured approach encompassing data exploration, preprocessing, model building, hyperparameter tuning, and advanced technique implementation, we aspire to develop a robust model that stands up to the competition standards and possibly exceeds them, thereby moving closer to solving the real-world problem of accurate price prediction in the peer-to-peer accommodation rental domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "The dataset provided for this project comprises information pertaining to accommodations listed on AirBnB, captured across 85 different attributes or columns. This dataset is housed within a Pandas DataFrame and totals 326,287 entries, extending from index 0 to 326,286. Below is a high-level summary of the dataset's structure and contained attributes:\n",
    "\n",
    "- **Entries:** 326,287\n",
    "- **Attributes:** 85\n",
    "- **Target Variable:** `Price`\n",
    "- **Data Types:** \n",
    "    - Integer: 2\n",
    "    - Float: 31\n",
    "    - Object: 52\n",
    "- **Memory Usage:** 211.6+ MB\n",
    "\n",
    "#### Attribute Highlights\n",
    "\n",
    "1. **Identifier Attributes:**\n",
    "   - `id`: Unique identifier for each listing.\n",
    "   - `Host ID`: Unique identifier for each host.\n",
    "\n",
    "2. **Textual Descriptions:**\n",
    "   - `Name`, `Summary`, `Description`: Textual descriptions of the listing.\n",
    "   - `Neighborhood Overview`, `Notes`, `Transit`: Additional textual information about the listing’s neighborhood and transit options.\n",
    "\n",
    "3. **Host Information:**\n",
    "   - `Host Name`, `Host Since`, `Host Location`: Information regarding the host.\n",
    "   - `Host Response Time`, `Host Response Rate`: Host’s responsiveness metrics.\n",
    "\n",
    "4. **Location and Property Attributes:**\n",
    "   - `Street`, `Neighbourhood`, `City`, `State`, `Country`: Location-related attributes.\n",
    "   - `Property Type`, `Room Type`: Descriptors of the property type and room type.\n",
    "\n",
    "5. **Accommodation Features:**\n",
    "   - `Accommodates`, `Bathrooms`, `Bedrooms`, `Beds`: Attributes indicating the accommodation capacity and facilities.\n",
    "   - `Amenities`: List of amenities provided.\n",
    "\n",
    "6. **Pricing and Booking Information:**\n",
    "   - `Price`, `Security Deposit`, `Cleaning Fee`: Pricing-related information.\n",
    "   - `Guests Included`, `Extra People`, `Minimum Nights`, `Maximum Nights`: Booking-related attributes.\n",
    "\n",
    "7. **Availability and Review Metrics:**\n",
    "   - `Availability 30`, `Availability 60`, `Availability 90`, `Availability 365`: Availability metrics over different time horizons.\n",
    "   - `Number of Reviews`, `Review Scores Rating`, `Reviews per Month`: Review-related metrics.\n",
    "\n",
    "8. **Miscellaneous:**\n",
    "   - `Features`: Other features of the listing.\n",
    "   - `Geolocation`: Geographical coordinates of the listing.\n",
    "\n",
    "This dataset presents a rich and diverse set of attributes, offering a substantial foundation upon which to build predictive models aimed at accurately estimating rental prices for AirBnB listings. The extensive variety of data attributes spans textual descriptions, categorical variables, numerical metrics, and date-related information, providing a well-rounded basis for a comprehensive exploratory data analysis (EDA) and subsequent model development.\n",
    "\n",
    "> The memory usage of this dataset is significant, amounting to over 211.6 MB, which necessitates efficient data handling and processing techniques to ensure smooth and effective model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "In this project, we will be leveraging the powerful capabilities of [Keras](https://keras.io/) to build and train our machine learning models. Keras is an open-source software library that provides a Python interface for artificial neural networks. It acts as an interface for the TensorFlow library, allowing for high-level building and training of models.\n",
    "\n",
    "To ensure that the necessary dependencies are correctly installed and managed throughout the project, we'll be utilizing [Conda](https://docs.conda.io/en/latest/) as our package manager. Conda is an open-source package management and environment management system that runs on Windows, macOS, and Linux.\n",
    "\n",
    "The `environment.yml` file located in the root of the project directory contains the list of all necessary packages and their respective versions required for this project. This file will allow us to create a Conda environment with the specified dependencies, ensuring a consistent environment across different setups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image Processing\n",
    "import imageio\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Concatenate,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping\n",
    ")\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting environment variables for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)  # Display all columns of a dataframe\n",
    "\n",
    "# set cuda lib path (probably not needed, this is for my local machine)\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/opt/cuda\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Random Seed for Reproducibility\n",
    "\n",
    "For any machine learning experiment, reproducibility is crucial. Setting a random seed ensures that the random numbers generated by our code are the same across different runs, making the results reproducible. In this project, the random seed is set for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 117\n",
    "# Set the seed for generating random numbers\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "In this section, we delve into the initial phase of our project where we load the dataset into our environment and perform an exploratory data analysis (EDA) to better understand the nature and characteristics of the data we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "The dataset for this project is conveniently provided and is located at the `/data` directory. Since this dataset it's too large to be uploaded to GitHub, it is not included in the repository. However, it can be downloaded from the [Kaggle competition page](https://www.kaggle.com/competitions/obligatorio-deep-learning-2023).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./data/public_train_data.csv\"\n",
    "SUBMISSION_PATH = \"./data/private_data_to_predict.csv\"\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "submission_df = pd.read_csv(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "The Exploratory Data Analysis (EDA) is a vital step that helps in understanding the intricacies of the data, spotting any anomalies, and uncovering patterns that could be instrumental in building a precise predictive model. The steps involved in the EDA for this dataset are outlined as follows:\n",
    "\n",
    "1. **Summary Statistics**\n",
    "   Acquiring summary statistics will provide insights into the central tendency and dispersion of the numerical attributes.\n",
    "\n",
    "2. **Data Type Analysis**\n",
    "   A review of the data types of each attribute to ensure they are in the correct format for analysis and modeling.\n",
    "\n",
    "3. **Missing Values Assessment**\n",
    "   Identifying and addressing missing values across different attributes to ensure completeness of the data.\n",
    "\n",
    "4. **Categorical Variable Analysis**\n",
    "   Exploring the unique values and counts of categorical variables to understand the distribution across different categories.\n",
    "\n",
    "5. **Correlation Analysis**\n",
    "   Analyzing the correlation between numerical variables, especially with respect to the target variable `Price`, to understand any strong relationships that might exist.\n",
    "\n",
    "6. **Text Data Overview**\n",
    "   Reviewing textual data to understand the quality and potential feature extraction opportunities it presents.\n",
    "\n",
    "Through a detailed EDA, the aim is to garner insights that will be pivotal in guiding the subsequent data preprocessing and model building stages, thereby ensuring a solid foundation for developing an accurate price prediction model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since we have too many attributes, we will only explore a subset of them. First we are going to remove (manually) the attributes that are not useful for our analysis (in my opinion). Then, we will select a subset of attributes that we will explore in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"Last Scraped\",\n",
    "    \"Experiences Offered\",\n",
    "    \"Thumbnail Url\",\n",
    "    \"Medium Url\",\n",
    "    \"Picture Url\",\n",
    "    \"XL Picture Url\",\n",
    "    \"Host URL\",\n",
    "    \"Host ID\",\n",
    "    \"Host Thumbnail Url\",\n",
    "    \"Host Name\",\n",
    "    \"Host Since\",\n",
    "    \"Host Location\",\n",
    "    \"Host About\",\n",
    "    \"Host Response Time\",\n",
    "    \"Host Thumbnail Url\",\n",
    "    \"Host Picture Url\",\n",
    "    \"Host Listings Count\",\n",
    "    \"Host Total Listings Count\",\n",
    "    \"Host Verifications\",\n",
    "    \"Calendar Updated\",\n",
    "    \"Calendar last Scraped\",\n",
    "    \"First Review\",\n",
    "    \"Last Review\",\n",
    "]\n",
    "train_df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics\n",
    "\n",
    "Summary statistics provide a high-level overview of the numerical attributes within the dataset, offering insights into the central tendency, dispersion, and shape of the distribution of the dataset, sans any influence of the other attributes. These statistics are crucial for understanding the typical behavior of the dataset, identifying outliers, and observing the distribution and spread of the data points across different attributes.\n",
    "\n",
    "Key Components of Summary Statistics:\n",
    "\n",
    "1. **Count:** The number of non-null entries for each attribute.\n",
    "2. **Mean:** The average value of each attribute, providing a measure of central tendency.\n",
    "3. **Standard Deviation (std):** A measure of the amount of variation or dispersion of a set of values.\n",
    "4. **Minimum (min) and Maximum (max):** The smallest and largest values in each attribute, respectively.\n",
    "5. **25th, 50th (median), and 75th Percentiles:** These values provide a summary of the distribution of values, where for instance, 25% of the data points are below the 25th percentile.\n",
    "\n",
    "A tabulated summary of these statistics can be procured for each numerical attribute in the dataset. This tabulated format allows for a clear, concise view of the dataset's overall behavior, and aids in identifying any potential anomalies or outliers that may require further investigation.\n",
    "\n",
    "Furthermore, summary statistics play a vital role in the data preprocessing stage, where understanding the distribution of data is crucial for tasks such as normalization, handling outliers, and feature scaling. By thoroughly analyzing these summary statistics, one can make informed decisions on the necessary preprocessing steps to enhance the model's performance in subsequent stages of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics = train_df.describe()\n",
    "\n",
    "# Displaying the summary statistics\n",
    "print(summary_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Type Analysis\n",
    "\n",
    "Analyzing the data types of each attribute is a crucial step in understanding the kind of data you are dealing with. This analysis helps in ensuring that each attribute is formatted correctly, which is essential for both data preprocessing and modeling stages of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values Assessment\n",
    "\n",
    "Assessing missing values is a critical step in the data exploration process. Missing data can lead to incorrect or biased analyses and conclusions. Identifying the presence and extent of missing values in the dataset is crucial to decide on the appropriate handling strategies.\n",
    "\n",
    "Implementing appropriate strategies to handle missing values is crucial to ensure the robustness and accuracy of the predictive model. The strategy chosen can significantly affect the model's performance and the insights derived from the data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a heatmap to visualize the missing values\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(train_df.isnull(), cbar=False, cmap=\"viridis\", yticklabels=False)\n",
    "plt.title(\"Missing values heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_NULL_THRESHOLD = 30\n",
    "\n",
    "# Calculate the percentage of missing values for each column\n",
    "missing_values_percentage = train_df.isnull().mean() * 100\n",
    "\n",
    "# Identify columns with missing values higher than the threshold\n",
    "columns_with_high_missing_values = missing_values_percentage[\n",
    "    missing_values_percentage > DROP_NULL_THRESHOLD\n",
    "].index\n",
    "\n",
    "# print the columns with high missing values\n",
    "for column in columns_with_high_missing_values:\n",
    "    print(f\"{column}: {missing_values_percentage[column]:.2f}% missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variable Analysis\n",
    "\n",
    "Categorical variable analysis is essential for understanding the distinct categories within attributes and their counts, which can provide insights into the data distribution and potential feature engineering opportunities.\n",
    "\n",
    "Key Steps:\n",
    "\n",
    "1. **Unique Value Identification:**\n",
    "   - Determine the unique values and their counts within each categorical attribute.\n",
    "\n",
    "2. **Visualization:**\n",
    "   - Use bar charts or pie charts to visualize the distribution of categorical variables.\n",
    "\n",
    "3. **Potential Encoding Strategies:**\n",
    "   - Identify appropriate encoding techniques like one-hot encoding or label encoding for handling categorical variables during the data preprocessing stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_TOP_N = 25\n",
    "categorical_attributes = [\n",
    "    \"Smart Location\",\n",
    "    \"City\",\n",
    "    \"State\",\n",
    "    \"Country Code\",\n",
    "    \"Property Type\",\n",
    "    \"Room Type\",\n",
    "    \"Bed Type\",\n",
    "]\n",
    "# Setting the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Loop through each categorical attribute and display the count of unique values\n",
    "for attribute in categorical_attributes:\n",
    "    # Get the count of each unique value\n",
    "    value_counts = train_df[attribute].value_counts()\n",
    "\n",
    "    # Display the count of unique values using a bar chart\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=value_counts.index[:SHOW_TOP_N], y=value_counts.values[:SHOW_TOP_N])\n",
    "    plt.title(f\"Distribution of {attribute}\")\n",
    "    plt.xticks(rotation=90)  # Rotate X-axis labels for better readability, if necessary\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values for each categorical attribute\n",
    "missing_values_categorical = train_df[categorical_attributes].isnull().sum()\n",
    "\n",
    "# Output the count of missing values\n",
    "print(missing_values_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Amerites\n",
    "This specific column is a list of amenities that the accommodation offers separeted by commas. We are going to create a new column for each amenity and assign a 1 if the accommodation offers it or a 0 if it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the comma-separated values in the 'Amenities' column\n",
    "amenities_split = train_df[\"Amenities\"].str.split(\",\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dictionary to hold the count of each amenity\n",
    "amenity_counts = {}\n",
    "\n",
    "# Iterating through each row and column of 'amenities_split'\n",
    "for index, row in amenities_split.iterrows():\n",
    "    for amenity in row:\n",
    "        if amenity and amenity not in amenity_counts:\n",
    "            amenity_counts[amenity] = 1\n",
    "        elif amenity:\n",
    "            amenity_counts[amenity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the 'amenity_counts' dictionary to a pandas Series for easier plotting\n",
    "amenity_counts_series = pd.Series(amenity_counts)\n",
    "\n",
    "# Sorting the counts in descending order for better visualization\n",
    "amenity_counts_series = amenity_counts_series.sort_values(ascending=False)\n",
    "\n",
    "# Creating a bar plot of the amenity counts\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=amenity_counts_series.index[:30], y=amenity_counts_series.values[:30])\n",
    "plt.title(\"Distribution of Amenities\")\n",
    "plt.xlabel(\"Amenities\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)  # Rotating the x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRELATION_THRESHOLD = 0.1\n",
    "\n",
    "# Select only numeric columns\n",
    "df_numeric = train_df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "# Get correlation of all features with 'price'\n",
    "price_correlation = correlation_matrix[\"Price\"].sort_values(ascending=False)\n",
    "\n",
    "# Filter out the features with a correlation above a certain threshold, for example 0.3\n",
    "important_numerical_features = price_correlation[\n",
    "    abs(price_correlation) >= CORRELATION_THRESHOLD\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(important_numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "\n",
    "Analyzing the correlation between numerical variables, especially with respect to the target variable `Price`, is integral to understanding any strong relationships that might exist. Correlation analysis provides insight into the linear relationships between variables, which is crucial for feature selection and predictive modeling.\n",
    "\n",
    "Visualizing correlations through heatmaps or scatter plots can provide a clear, intuitive view of the relationships between variables, making it easier to identify patterns and potential issues that might require further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_variables = important_numerical_features.index\n",
    "# Create a sub-matrix of the correlation matrix with only the important variables\n",
    "important_corr_matrix = train_df[important_variables].corr()\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Creating a heatmap to visualize the important correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(important_corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Important Correlations Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text_attributes = [\n",
    "    \"Name\",\n",
    "    \"Summary\",\n",
    "    \"Space\",\n",
    "    \"Description\",\n",
    "    \"Neighborhood Overview\",\n",
    "    \"Notes\",\n",
    "    \"Transit\",\n",
    "    \"Access\",\n",
    "    \"Interaction\",\n",
    "    \"House Rules\",\n",
    "]\n",
    "text_stats = train_df[plain_text_attributes].describe()\n",
    "print(text_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Data preprocessing is a critical step in the machine learning pipeline. It involves cleaning and transforming raw data into a format that can be used to train machine learning models effectively. The objective is to create high-quality data that boosts the performance of models by reducing potential sources of error and bias. Here's an overview of the key steps involved in data preprocessing:\n",
    "\n",
    "1. **Handling Missing Values:**\n",
    "   - Identify and fill or drop missing values to ensure a complete dataset. Strategies may include mean imputation, median imputation, mode imputation, or utilizing algorithms that handle missing values like K-Nearest Neighbors.\n",
    "\n",
    "2. **Encoding Categorical Variables:**\n",
    "   - Convert categorical variables into a numerical format through encoding techniques like one-hot encoding, label encoding, or binary encoding.\n",
    "\n",
    "3. **Text Preprocessing:**\n",
    "   - For textual data, perform cleaning, tokenization, stemming/lemmatization, and vectorization to convert text into a format suitable for machine learning.\n",
    "\n",
    "4. **Data Splitting:**\n",
    "   - Split the data into training, validation, and testing sets to evaluate the performance of models accurately.\n",
    "\n",
    "5. **Outlier Detection:**\n",
    "    - Identify and handle outliers which can adversely affect model performance.\n",
    "\n",
    "Each of these steps requires careful consideration and a good understanding of the data and the problem at hand. The preprocessing steps taken can significantly affect the outcome of the final models, and therefore, a well-thought-out preprocessing strategy is crucial for building robust and high-performing machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picking attributes for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_attributes = (\n",
    "    important_numerical_features.index.tolist()\n",
    ")  # based on correlation\n",
    "numerical_attributes.remove(\"Price\")\n",
    "\n",
    "categorical_attributes = categorical_attributes  # based on EDA\n",
    "categorical_attributes.remove(\n",
    "    \"City\"\n",
    ")  # removed beacuse it has NaN values and Smart Location has the same information (and populated)\n",
    "categorical_attributes.remove(\"State\")  # same as above\n",
    "\n",
    "text_attributes = [\"Name\", \"Summary\", \"Space\"]\n",
    "\n",
    "coordenate_attributes = [\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "]  # why not? lat and long are numerical attributes and they are correlated with price\n",
    "\n",
    "train_df = train_df[\n",
    "    numerical_attributes\n",
    "    + categorical_attributes\n",
    "    + [\"Amenities\"]\n",
    "    + text_attributes\n",
    "    + coordenate_attributes\n",
    "    + [\"Price\"]\n",
    "]\n",
    "train_df.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "submission_df = submission_df[\n",
    "    [\"id\"]\n",
    "    + numerical_attributes\n",
    "    + categorical_attributes\n",
    "    + [\"Amenities\"]\n",
    "    + text_attributes\n",
    "    + coordenate_attributes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Values\n",
    "\n",
    "##### Numerical Variables\n",
    "\n",
    "In our dataset, there are a few columns with a substantial amount of missing values that need addressing to ensure the robustness and accuracy of our future models. The columns of interest are:\n",
    "\n",
    "1. **Square Feet:** \n",
    "   - Missing Rate: 97.55%\n",
    "   - This column has a high rate of missing values, which could be due to the lack of available data or it wasn't applicable for certain listings. Given the high missing rate, imputing values might introduce a lot of noise or bias. However, the fact that the square footage is missing could be informative in itself.\n",
    "\n",
    "2. **Security Deposit:** \n",
    "   - Missing Rate: 58.28%\n",
    "   - The missing values in this column could be indicative of listings that do not require a security deposit, or it might be data that wasn't collected or provided.\n",
    "\n",
    "3. **Cleaning Fee:** \n",
    "   - Missing Rate: 36.20%\n",
    "   - Similar to the Security Deposit column, missing values here could be due to listings that do not have a cleaning fee or missing data collection.\n",
    "\n",
    "Proposed Strategies:\n",
    "\n",
    "1. **Square Feet, Security Deposit and Cleaning Fee:** \n",
    "   - I will add a binary \"missing\" indicator to this column, where `1` indicates a missing value and `0` indicates a non-missing value.\n",
    "   > I tried to use KNN imputation, but it was too slow.\n",
    "\n",
    "2. **Other Numerical Variables:**\n",
    "   - For the remaining numerical variables, we'll use mean imputation to fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a missing indicator for Square Feet and Security Deposit\n",
    "\n",
    "train_df[\"Square_Feet_Missing\"] = train_df[\"Square Feet\"].isnull().astype(int)\n",
    "train_df[\"Square Feet\"].fillna(0, inplace=True)  # Fill missing values with 0\n",
    "\n",
    "train_df[\"Security_Deposit_Missing\"] = train_df[\"Security Deposit\"].isnull().astype(int)\n",
    "train_df[\"Security Deposit\"].fillna(0, inplace=True)  # Fill missing values with 0\n",
    "\n",
    "train_df[\"Cleaning_Fee_Missing\"] = train_df[\"Cleaning Fee\"].isnull().astype(int)\n",
    "train_df[\"Cleaning Fee\"].fillna(0, inplace=True)  # Fill missing values with 0\n",
    "\n",
    "\n",
    "submission_df[\"Square_Feet_Missing\"] = submission_df[\"Square Feet\"].isnull().astype(int)\n",
    "submission_df[\"Square Feet\"].fillna(0, inplace=True)  # Fill missing values with 0\n",
    "\n",
    "submission_df[\"Security_Deposit_Missing\"] = (\n",
    "    submission_df[\"Security Deposit\"].isnull().astype(int)\n",
    ")\n",
    "submission_df[\"Security Deposit\"].fillna(0, inplace=True)  # Fill missing values with 0\n",
    "\n",
    "submission_df[\"Cleaning_Fee_Missing\"] = (\n",
    "    submission_df[\"Cleaning Fee\"].isnull().astype(int)\n",
    ")\n",
    "submission_df[\"Cleaning Fee\"].fillna(0, inplace=True)  # Fill missing values with 0\n",
    "\n",
    "missing_indicator_attributes = [\n",
    "    \"Square_Feet_Missing\",\n",
    "    \"Security_Deposit_Missing\",\n",
    "    \"Cleaning_Fee_Missing\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to impute\n",
    "columns_to_impute = numerical_attributes\n",
    "columns_to_impute.remove(\"Square Feet\")\n",
    "columns_to_impute.remove(\"Security Deposit\")\n",
    "columns_to_impute.remove(\"Cleaning Fee\")\n",
    "\n",
    "# Impute missing values with the median of each column\n",
    "for column in columns_to_impute:\n",
    "    median_value = train_df[column].median()\n",
    "    train_df[column].fillna(median_value, inplace=True)\n",
    "    submission_df[column].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Variables\n",
    "\n",
    "Missing values in categorical variables can introduce bias, lead to misinterpretation, and ultimately, incorrect analysis. Therefore, understanding and appropriately handling missing values is a critical step in preparing our data for analysis or predictive modeling. In our dataset, we have identified a few categorical variables with missing values. The variables and the chosen strategies to handle their missing values are detailed below:\n",
    "\n",
    "1. **Amenities**:\n",
    "   The `Amenities` column contains comma-separated values representing various amenities provided by the Airbnb listings. Missing values in this column suggest that the listing does not provide any additional amenities. Hence, we will replace missing values in the `Amenities` column with an empty string (\"\").\n",
    "\n",
    "These strategies are selected based on the nature of the data, the context, and the potential impact on the analysis or predictive modeling process. Implementing these strategies will ensure that our dataset is well-prepared for the subsequent steps in our project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values in Amenities column by replacing NaN with empty string\n",
    "train_df[\"Amenities\"].fillna(\"\", inplace=True)\n",
    "submission_df[\"Amenities\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Variables\n",
    "\n",
    "In our dataset, we have several text variables that provide descriptive information about the listings. These textual attributes could hold valuable insights for our analysis or predictive modeling. However, missing or null values in these text variables can pose challenges in processing and analyzing them.\n",
    "\n",
    "To streamline the handling of missing values in these text variables, we have decided to replace any missing or null values with an empty string (`\"\"`). This approach will ensure consistency across our dataset and facilitate easier text processing down the line. Replacing missing text data with an empty string is a straightforward way to handle missing values in textual data and provides a clear indication of the absence of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in text_attributes:\n",
    "    train_df[column].fillna(\"\", inplace=True)\n",
    "    submission_df[column].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Variables\n",
    "\n",
    "To prepare our categorical data for the deep learning models, we will employ different encoding strategies based on the nature and cardinality of each categorical attribute:\n",
    "\n",
    "1. **Embedding Layers**:\n",
    "   - We will utilize embedding layers for the high cardinality categorical attributes. Embedding layers are proficient in transforming categorical data into a continuous representation while preserving the categorical semantics. They are particularly beneficial in the context of deep learning models like DNN and RNN.\n",
    "   - Attributes to be encoded using embedding layers:\n",
    "     - `Smart Location`\n",
    "\n",
    "2. **One-Hot Encoding**:\n",
    "   - For categorical attributes with a lower number of unique values, one-hot encoding is a straightforward and effective approach. It creates a binary column for each category which is suitable for our model.\n",
    "   - Attributes to be encoded using one-hot encoding:\n",
    "     - `Country Code`\n",
    "     - `Property Type`\n",
    "     - `Room Type`\n",
    "     - `Bed Type`\n",
    "\n",
    "3. **Amenities Attribute Splitting and One-Hot Encoding**:\n",
    "   - The `Amenities` attribute contains multiple categorical values separated by commas for each observation. We will split these values into individual categories, and then apply one-hot encoding to create binary columns for each unique amenity.\n",
    "   - This approach will help in capturing the presence or absence of specific amenities for each listing, which could be a significant factor in predicting the price of the listing.\n",
    "\n",
    "These encoding strategies will help in transforming the categorical data into a suitable numerical format for our deep learning models, ensuring that the categorical semantics are well-represented in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare for Embedding Layers\n",
    "encoder = LabelEncoder()\n",
    "# Concatenate the 'Smart Location' columns from both dataframes (in case there are new locations in the submission dataframe)\n",
    "all_locations = pd.concat([train_df[\"Smart Location\"], submission_df[\"Smart Location\"]])\n",
    "# Fit the encoder on all locations\n",
    "encoder.fit(all_locations)\n",
    "\n",
    "# Now transform the 'Smart Location' columns in train_df and submission_df\n",
    "train_df[\"Smart_Location_encoded\"] = encoder.transform(train_df[\"Smart Location\"])\n",
    "submission_df[\"Smart_Location_encoded\"] = encoder.transform(\n",
    "    submission_df[\"Smart Location\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_locations = len(all_locations.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. One-Hot Encoding for specified columns\n",
    "one_hot_cols = [\"Country Code\", \"Property Type\", \"Room Type\", \"Bed Type\"]\n",
    "train_df = pd.get_dummies(\n",
    "    train_df, columns=one_hot_cols, prefix=one_hot_cols, drop_first=True, dtype=int\n",
    ")\n",
    "submission_df = pd.get_dummies(\n",
    "    submission_df, columns=one_hot_cols, prefix=one_hot_cols, drop_first=True, dtype=int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the one-hot encoded columns\n",
    "one_hot_attributes = [\n",
    "    col for col in train_df.columns if col.startswith(tuple(one_hot_cols))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Splitting and One-Hot Encoding for Amenities attribute\n",
    "# Splitting the amenities into a list of amenities\n",
    "train_df[\"Amenities\"] = train_df[\"Amenities\"].str.split(\",\")\n",
    "submission_df[\"Amenities\"] = submission_df[\"Amenities\"].str.split(\",\")\n",
    "\n",
    "# Applying one-hot encoding to the list of amenities using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_amenities_encoded = mlb.fit_transform(train_df[\"Amenities\"])\n",
    "submission_amenities_encoded = mlb.transform(submission_df[\"Amenities\"])\n",
    "\n",
    "# Creating dataframes from the encoded amenities data\n",
    "# Creating dataframes from the encoded amenities data\n",
    "train_amenities_df = pd.DataFrame(\n",
    "    train_amenities_encoded,\n",
    "    columns=[\"Amenities_\" + col for col in mlb.classes_],\n",
    "    index=train_df.index,\n",
    ")\n",
    "submission_amenities_df = pd.DataFrame(\n",
    "    submission_amenities_encoded,\n",
    "    columns=[\"Amenities_\" + col for col in mlb.classes_],\n",
    "    index=submission_df.index,\n",
    ")\n",
    "\n",
    "# The rest of your code remains the same...\n",
    "\n",
    "# Concatenating the encoded amenities dataframes with the original dataframes\n",
    "train_df = pd.concat([train_df, train_amenities_df], axis=1)\n",
    "submission_df = pd.concat([submission_df, submission_amenities_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "\n",
    "1. **Concatenating Text Attributes**:\n",
    "   - Merge all text attributes into a single column.\n",
    "   - Limit the text in each column to a specified `MAX_LEN` before merging to avoid excessively long text strings.\n",
    "\n",
    "2. **Tokenization**:\n",
    "   - Split the merged text into individual words, which will enable further processing such as stopword removal and lemmatization.\n",
    "\n",
    "3. **Stopword Removal**:\n",
    "   - Remove common words (e.g., \"the\", \"a\", \"an\") that do not carry much meaning and are often removed in the preprocessing step.\n",
    "\n",
    "4. **Lemmatization**:\n",
    "   - Reduce words to their root or base form (e.g., \"running\" to \"run\").\n",
    "\n",
    "5. **Vectorization (Word2Vec)**:\n",
    "   - Convert words into vectors of numbers to allow for analysis and modeling. Word2Vec is a popular method that can capture semantic relationships between words.\n",
    "\n",
    "The above steps aim to prepare the text data for further analysis and modeling, particularly when looking to use this data in conjunction with numerical and categorical data for Deep Learning models. Each step of the preprocessing will clean and transform the text data into a more usable format for the downstream tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Concatenating Text Attributes\n",
    "MAX_LEN = 50\n",
    "\n",
    "# Limit text in each column to MAX_LEN\n",
    "for column in text_attributes:\n",
    "    train_df[column] = train_df[column].str.slice(0, MAX_LEN)\n",
    "    submission_df[column] = submission_df[column].str.slice(0, MAX_LEN)\n",
    "\n",
    "# Concatenate text columns into a single column\n",
    "train_df[\"merged_text\"] = train_df[text_attributes].apply(lambda x: \" \".join(x), axis=1)\n",
    "submission_df[\"merged_text\"] = submission_df[text_attributes].apply(\n",
    "    lambda x: \" \".join(x), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all text\n",
    "train_df[\"merged_text\"] = train_df[\"merged_text\"].str.lower()\n",
    "submission_df[\"merged_text\"] = submission_df[\"merged_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenization\n",
    "import nltk\n",
    "\n",
    "# Tokenize the merged_text column\n",
    "train_df[\"tokenized_text\"] = train_df[\"merged_text\"].apply(nltk.word_tokenize)\n",
    "submission_df[\"tokenized_text\"] = submission_df[\"merged_text\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Stopword Removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove stopwords from the tokenized text\n",
    "train_df[\"text_without_stopwords\"] = train_df[\"tokenized_text\"].apply(\n",
    "    lambda x: [word for word in x if word.lower() not in stop_words]\n",
    ")\n",
    "submission_df[\"text_without_stopwords\"] = submission_df[\"tokenized_text\"].apply(\n",
    "    lambda x: [word for word in x if word.lower() not in stop_words]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each word in the text\n",
    "train_df[\"lemmatized_text\"] = train_df[\"text_without_stopwords\"].apply(\n",
    "    lambda x: [lemmatizer.lemmatize(word) for word in x]\n",
    ")\n",
    "submission_df[\"lemmatized_text\"] = submission_df[\"text_without_stopwords\"].apply(\n",
    "    lambda x: [lemmatizer.lemmatize(word) for word in x]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 100\n",
    "\n",
    "# Step 5: Vectorization (Word2Vec)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Create Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=train_df[\"lemmatized_text\"],\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    ")\n",
    "\n",
    "\n",
    "def vectorize_text(tokens):\n",
    "    vector_sum = sum(model.wv[word] for word in tokens if word in model.wv)\n",
    "    return vector_sum\n",
    "\n",
    "\n",
    "# Apply vectorization to each row of text data in training and testing datasets\n",
    "train_df[\"vectorized_text\"] = train_df[\"lemmatized_text\"].apply(vectorize_text)\n",
    "submission_df[\"vectorized_text\"] = submission_df[\"lemmatized_text\"].apply(\n",
    "    vectorize_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some rows have no text, so they are not vectorized\n",
    "# we are going to fill them with vector (VECTOR_SIZE) of zeros\n",
    "\n",
    "train_non_vector_indices = train_df.index[\n",
    "    train_df[\"vectorized_text\"].apply(type) != np.ndarray\n",
    "]\n",
    "submission_non_vector_indices = submission_df.index[\n",
    "    submission_df[\"vectorized_text\"].apply(type) != np.ndarray\n",
    "]\n",
    "\n",
    "train_df.loc[train_non_vector_indices, \"vectorized_text\"] = train_df.loc[\n",
    "    train_non_vector_indices, \"vectorized_text\"\n",
    "].apply(lambda x: np.zeros(VECTOR_SIZE))\n",
    "\n",
    "submission_df.loc[submission_non_vector_indices, \"vectorized_text\"] = submission_df.loc[\n",
    "    submission_non_vector_indices, \"vectorized_text\"\n",
    "].apply(lambda x: np.zeros(VECTOR_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier\n",
    "\n",
    "I have opted not to exclude outliers from the dataset as these values represent genuine data points. This decision aligns with perspectives shared on [Stack Exchange](https://stats.stackexchange.com/questions/298551/will-removing-outliers-improve-my-predictive-model), suggesting that removing outliers can sometimes lead to the loss of valuable information, which could potentially improve the predictive accuracy of a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=train_df[\"Price\"])\n",
    "plt.show()\n",
    "\n",
    "# To see a more detailed distribution\n",
    "sns.histplot(train_df[\"Price\"], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing columns in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get missing columns in the submission set\n",
    "missing_cols = set(train_df.columns) - set(submission_df.columns)\n",
    "\n",
    "# Add a missing column in submission set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    submission_df[c] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Inputs\n",
    "\n",
    "In this section, we set up various input layers, each designed to handle distinct data types and structures in our dataset:\n",
    "\n",
    "1. **Numerical Attributes Input**: Handles the processing of straightforward numerical data, capturing quantitative aspects of our dataset.\n",
    "\n",
    "2. **Missing Indicator Attributes Input**: Dedicated to identifying and managing missing values in the data, ensuring overall data integrity.\n",
    "\n",
    "3. **One-Hot Encoding Attributes Input**: Transforms categorical data into a one-hot encoded format, making it easier for the model to process non-numerical information.\n",
    "\n",
    "4. **Amenities Attributes Input**: Tailored to manage the amenities data, this layer plays a crucial role in detailed analysis and insights.\n",
    "\n",
    "5. **Smart Location Embedding**: Incorporates an embedding layer for 'Smart Location' data, converting this attribute into a format that is meaningful and easier for the model to process.\n",
    "\n",
    "6. **Coordinates Attributes Input**: Processes geographical data, such as coordinates, adding spatial context to the model's understanding.\n",
    "\n",
    "7. **Word2Vec Vectorized Text Input**: Manages the text data vectorized using Word2Vec, ensuring the model can effectively interpret and learn from textual content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 512\n",
    "\n",
    "# Numerical attributes\n",
    "numerical_input = Input(shape=(len(numerical_attributes),), name=\"numerical_input\")\n",
    "\n",
    "# Missing indicator attributes\n",
    "missing_indicator_input = Input(\n",
    "    shape=(len(missing_indicator_attributes),), name=\"missing_indicator_input\"\n",
    ")\n",
    "\n",
    "# One hot encoding attributes\n",
    "one_hot_input = Input(shape=(len(one_hot_attributes),), name=\"one_hot_input\")\n",
    "\n",
    "# Amenities attributes\n",
    "amenities_input = Input(\n",
    "    shape=(len(train_amenities_df.columns),), name=\"amenities_input\"\n",
    ")\n",
    "\n",
    "# Embedding for 'Smart Location'\n",
    "smart_location_input = Input(shape=(1,), name=\"smart_location_input\")\n",
    "smart_location_embedding = Embedding(\n",
    "    input_dim=number_of_locations,\n",
    "    output_dim=EMBEDDING_DIMENSION,\n",
    "    name=\"smart_location_embedding\",\n",
    ")(smart_location_input)\n",
    "smart_location_flat = Flatten()(smart_location_embedding)\n",
    "\n",
    "# Coordinates attributes\n",
    "coordinates_input = Input(shape=(len(coordenate_attributes),), name=\"coordinates_input\")\n",
    "\n",
    "# Word2Vec Vectorized Text\n",
    "vectorized_text_input = Input(shape=(VECTOR_SIZE,), name=\"vectorized_text_input\")\n",
    "\n",
    "\n",
    "all_inputs = [\n",
    "    numerical_input,\n",
    "    missing_indicator_input,\n",
    "    one_hot_input,\n",
    "    amenities_input,\n",
    "    smart_location_input,\n",
    "    coordinates_input,\n",
    "    vectorized_text_input,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model architecture leverages a combination of wide and deep branches to efficiently process various types of inputs. Here's a breakdown of the approach:\n",
    "\n",
    "- **Wide Branch**: This part of the model directly concatenates simpler features like numerical attributes, missing indicators, and one-hot encoded data. It's designed to capture straightforward relationships without complex transformations.\n",
    "\n",
    "- **Deep Branches**: These are dedicated to more complex inputs requiring deeper processing, like embedded locations and vectorized text. Each input is fed through dense layers, regularized with dropout, and normalized with batch normalization.\n",
    "\n",
    "- **Combination and Processing**: The wide and deep branches are then concatenated. Sequential dense layers are added, with dropout and batch normalization applied between them for regularization and normalization, respectively.\n",
    "\n",
    "- **Output Layer**: The final output is a single dense layer with a ReLU activation function, tailored for price prediction.\n",
    "\n",
    "This architecture offers a blend of simplicity for easily interpretable features and depth for more complex representations, making it suitable for a diverse set of inputs.\n",
    "\n",
    "> One of the requirements for this work it's to use [Batch Normalization](https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(neurons_per_layer, activation_function, dropout_rate):\n",
    "\n",
    "    # Wide branch\n",
    "    wide_branch = Concatenate()([numerical_input, missing_indicator_input, one_hot_input, amenities_input])\n",
    "\n",
    "    # Deep branch - Sequentially add dense layers for each input that requires deep processing\n",
    "    deep_branches = []\n",
    "    for input_tensor, units in zip(\n",
    "            [smart_location_flat, coordinates_input, vectorized_text_input],\n",
    "            [256, 256, 256]\n",
    "    ):\n",
    "        branch = Dense(units, activation=activation_function)(input_tensor)\n",
    "        branch = Dropout(dropout_rate)(branch)  # Regularization\n",
    "        branch = BatchNormalization()(branch)  # Normalize the activations\n",
    "        deep_branches.append(branch)\n",
    "\n",
    "    # Combine the deep branches\n",
    "    deep_branch = Concatenate()(deep_branches)\n",
    "\n",
    "    # Final model\n",
    "    combined = Concatenate()([wide_branch, deep_branch])\n",
    "\n",
    "    for i in range(len(neurons_per_layer)):\n",
    "        combined = Dense(\n",
    "            units=neurons_per_layer[i],\n",
    "            activation=activation_function,\n",
    "        )(combined)\n",
    "        if i != len(neurons_per_layer) - 1:\n",
    "            combined = Dropout(dropout_rate)(combined)\n",
    "            combined = BatchNormalization()(combined) \n",
    "\n",
    "    output = Dense(1, activation='relu')(combined)  # Price prediction\n",
    "\n",
    "    return all_inputs, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation\n",
    "\n",
    "The model compilation step is crucial in preparing the model for training. It involves specifying the optimizer, learning rate, and loss function. Here's how it's set up:\n",
    "\n",
    "- **Optimizer Selection**: The model supports either Adam or SGD optimizers, with an added feature of gradient clipping. Gradient clipping is a technique to prevent exploding gradients, as recommended by [Machine Learning Mastery](https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/). It limits the size of the gradient during backpropagation to maintain stable training.\n",
    "\n",
    "- **Model Configuration**: After defining the optimizer, the model is compiled with the chosen loss function. The metrics Mean Absolute Error (MAE) and Mean Squared Error (MSE) are used for monitoring performance.\n",
    "\n",
    "This setup ensures that the model is optimized for accurate and stable learning, addressing potential issues like exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(layers, output_layer, optimizer, learning_rate, clip_value, loss):\n",
    "    if optimizer == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate, clipvalue=clip_value\n",
    "        )\n",
    "    elif optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate, clipvalue=clip_value\n",
    "        )\n",
    "\n",
    "    model = Model(inputs=layers, outputs=output_layer)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Model training is a critical phase where the neural network learns from the data. Here's an overview of the process:\n",
    "\n",
    "- **Data Preparation**: The function `get_x_y` processes the DataFrame to generate the input features (X) and target values (y). It handles various data types, including numerical, categorical, and text data.\n",
    "\n",
    "- **Model Initialization and Configuration**: The model is initialized with parameters such as layer architecture, activation functions, and dropout rates, configured dynamically using Weights & Biases (wandb).\n",
    "\n",
    "- **Training Process**: The model is trained using the `fit` method. It employs callbacks for monitoring (WandbCallback) and early stopping to prevent overfitting. The training uses a configurable number of epochs, batch size, and validation split. `max_queue_size` and `workers` parameters manage the loading of data, beneficial for parallelizing data loading and processing.\n",
    "\n",
    "- **Error Handling**: The training function includes exception handling to log and gracefully exit in case of errors.\n",
    "\n",
    "This approach ensures a flexible, monitored, and efficient training process, adapting to various configurations and dataset complexities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y(df):\n",
    "    X1 = df[numerical_attributes].values\n",
    "    X2 = df[missing_indicator_attributes].values\n",
    "    X3 = df[one_hot_attributes].values\n",
    "    X4 = df[train_amenities_df.columns.tolist()].values\n",
    "    X5 = df[\"Smart_Location_encoded\"].values\n",
    "    X6 = df[coordenate_attributes].values\n",
    "\n",
    "    # Convert these sequences to a list\n",
    "    X7_list = df[\"vectorized_text\"].tolist()\n",
    "\n",
    "    # Now convert the padded sequences to a tensor\n",
    "    X7_tensor = tf.convert_to_tensor(X7_list, dtype=tf.float32)\n",
    "\n",
    "    X = [X1, X2, X3, X4, X5, X6, X7_tensor]\n",
    "\n",
    "    y = df[\"Price\"].values\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import sys\n",
    "import wandb\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "\n",
    "def run_train():\n",
    "    try:\n",
    "        with wandb.init(config=None, project=project, entity=entity):\n",
    "            config = wandb.config\n",
    "            print(config)\n",
    "            # input and output layers\n",
    "            input_layer, output_layer = get_layers(\n",
    "                config.neurons_per_layer,\n",
    "                config.activation_function,\n",
    "                config.dropout_rate,\n",
    "            )\n",
    "            # get X and y\n",
    "            X_train, y_train = get_x_y(train_df)\n",
    "            # get model\n",
    "            model = get_model(\n",
    "                input_layer,\n",
    "                output_layer,\n",
    "                config.optimizer,\n",
    "                config.learning_rate,\n",
    "                config.clip_value,\n",
    "                config.loss,\n",
    "            )\n",
    "            # clear session (cache)\n",
    "            tf.keras.backend.clear_session()\n",
    "            # callbacks\n",
    "            wandb_callback = wandb.keras.WandbCallback()\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "            # fit model\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=config.batch_size,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[wandb_callback, early_stopping],\n",
    "                max_queue_size=3,\n",
    "                workers=8,\n",
    "            )\n",
    "    except Exception as e:\n",
    "        # exit gracefully, so wandb logs the problem\n",
    "        print(traceback.print_exc(), file=sys.stderr)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Setup\n",
    "\n",
    "Hyperparameter tuning is essential for optimizing model performance. We use [Weights & Biases](https://wandb.ai/) (wandb) for this purpose, setting up a sweep configuration. Here's an overview:\n",
    "\n",
    "- **Sweep Configuration**: Defines the hyperparameters to be tuned, including the loss function, batch size, neurons per layer, dropout rate, activation function, optimizer, learning rate, and gradient clipping value. \n",
    "\n",
    "- **Bayesian Optimization**: The 'bayes' method is employed for efficient searching through the hyperparameter space, guided by past trial results to make smarter decisions.\n",
    "\n",
    "- **Metric**: The mean absolute error (MAE) is chosen as the metric to minimize, focusing on average prediction accuracy.\n",
    "\n",
    "- **Parameter Range**: Each hyperparameter has specified ranges or sets of values. For example, the batch size varies between 32 to 1024, and the learning rate is explored between 0.00005 to 0.002.\n",
    "\n",
    "- **Weights & Biases**: Wandb provides a robust platform for hyperparameter tuning, offering visualization and logging capabilities. This setup allows for a systematic and informed approach to finding the best model configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"sweep_dreams_are_made_of_this\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"mae\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"loss\": {\n",
    "            \"values\": [\"mse\", \"mae\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"max\": 1024,\n",
    "            \"min\": 32,\n",
    "            \"distribution\": \"int_uniform\"\n",
    "        },\n",
    "        \"neurons_per_layer\": {\n",
    "            \"values\": [\n",
    "                [512, 256, 128],\n",
    "                [256, 128, 64],\n",
    "                [512, 256],\n",
    "                [256, 128],\n",
    "            ]\n",
    "        },\n",
    "        \"dropout_rate\": {\n",
    "            \"max\": 0.9,\n",
    "            \"min\": 0.05,\n",
    "            \"distribution\": \"uniform\"\n",
    "        },\n",
    "        \"activation_function\": {\n",
    "            \"values\": [\"relu\", \"tanh\"],\n",
    "            \"distribution\": \"categorical\"\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\", \"sgd\"],\n",
    "            \"distribution\": \"categorical\"\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"max\": 0.002,\n",
    "            \"min\": 0.00005,\n",
    "            \"distribution\": \"uniform\"\n",
    "        },\n",
    "        \"clip_value\": {\n",
    "            \"max\": 5,\n",
    "            \"min\": 0.05,\n",
    "            \"distribution\": \"uniform\"\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution of Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "PROJECT = \"obligatorio_dl\"\n",
    "ENTITY = \"joaquin-vigna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = 100\n",
    "\n",
    "SWEEP_ID = wandb.sweep(sweep_config, project=PROJECT, entity=ENTITY)\n",
    "\n",
    "wandb.agent(SWEEP_ID, function=run_train, count=RUNS, project=PROJECT, entity=ENTITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Grid Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Grid Search Results](assets/grid_search.png)\n",
    "![MAE](assets/mae.png)\n",
    "![VAL MAE](assets/val_mae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAE and val_MAE trends are good; both decrease indicating learning. No overfitting is apparent as trends align. Some runs plateau early (early stopping).\n",
    "\n",
    "You can see the complete results [here](https://wandb.ai/joaquin-vigna/obligatorio_dl/sweeps/k4q58ml3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieveng the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "SWEEP_ID = \"k4q58ml3\"\n",
    "\n",
    "# Initialize the wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Get the sweep\n",
    "sweep = api.sweep(f\"{ENTITY}/{PROJECT}/sweeps/{SWEEP_ID}\")\n",
    "\n",
    "# Retrieve the best run\n",
    "best_run = sweep.best_run()\n",
    "\n",
    "print(\"Best Run ID:\", best_run.id)\n",
    "print(\"Best Run Metrics:\", best_run.summary)\n",
    "\n",
    "best_run.file(\"model-best.h5\").download(replace=True)\n",
    "\n",
    "# # Load the model\n",
    "model = load_model(\"model-best.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation in Kaggle Competition\n",
    "### Submission Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, _ =  get_x_y(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_results = model.predict(X_test)\n",
    "test_ids = submission_df['id']\n",
    "test_ids = np.array(test_ids).reshape(-1,1)\n",
    "output = np.stack((test_ids, kaggle_results), axis=-1)\n",
    "output = output.reshape([-1, 2])\n",
    "df = pd.DataFrame(output)\n",
    "df.columns = ['id','expected']  \n",
    "df['expected'] = df['expected'].fillna(0)   \n",
    "df.to_csv(\"output_to_submit.csv\", index = False, index_label = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kaggle](assets/kaggle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achieving a score of 45 on Kaggle, significantly below the target of 70, indicates a strong performance of the model in the competition. This result demonstrates effective feature engineering, model selection, and hyperparameter tuning, leading to a highly accurate predictive model. The accomplishment showcases the proficiency in applying machine learning techniques to real-world problems, reflecting well on both the model's design and the training approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This journey through machine learning, from data preprocessing to complex model architecture, showcases a deep understanding of AI principles and practical application. The successful Kaggle submission, underscored by a score well below the target, highlights the effectiveness of the chosen methods. The project's progression, including tackling challenges like hyperparameter tuning and model optimization, illustrates a thorough grasp of AI strategies. This endeavor not only achieves its immediate goal but also sets a solid foundation for future explorations and innovations in the field of artificial intelligence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taller_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
