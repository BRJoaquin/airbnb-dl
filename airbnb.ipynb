{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AIRBNB](https://www.stevenridercpa.au/wp-content/uploads/2022/09/airbnb-tax.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb - Price Prediction\n",
    "-------\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](##Introduction)\n",
    "    - [Problem Statement](###Problem-Statement)\n",
    "    - [Objective](###Objective)\n",
    "    - [Dataset Overview](###Dataset-Overview)\n",
    "2. [Setup](##Setup)\n",
    "2. [Data Loading and Exploration](##Data-Loading-and-Exploration)\n",
    "    - [Loading the Dataset](###Loading-the-Dataset)\n",
    "    - [Exploratory Data Analysis (EDA)](###Exploratory-Data-Analysis-(EDA))\n",
    "    - [Data Preprocessing](###Data-Preprocessing)\n",
    "3. [Baseline Model](##Baseline-Model)\n",
    "    - [Model Architecture](###Model-Architecture)\n",
    "    - [Model Compilation](###Model-Compilation)\n",
    "    - [Model Training](###Model-Training)\n",
    "    - [Model Evaluation](###Model-Evaluation)\n",
    "4. [Hyperparameter Tuning](##Hyperparameter-Tuning)\n",
    "    - [Grid Search Setup](###Grid-Search-Setup)\n",
    "    - [Execution of Grid Search](###Execution-of-Grid-Search)\n",
    "    - [Analysis of Grid Search Results](###Analysis-of-Grid-Search-Results)\n",
    "5. [Advanced Techniques Implementation](##Advanced-Techniques-Implementation)\n",
    "    - [Batch Normalization](###Batch-Normalization)\n",
    "    - [Gradient Normalization and/or Gradient Clipping](###Gradient-Normalization-and/or-Gradient-Clipping)\n",
    "6. [Final Model](##Final-Model)\n",
    "    - [Model Architecture](###Model-Architecture)\n",
    "    - [Model Compilation](###Model-Compilation)\n",
    "    - [Model Training](###Model-Training)\n",
    "    - [Model Evaluation](###Model-Evaluation)\n",
    "7. [Participation in Kaggle Competition](##Participation-in-Kaggle-Competition)\n",
    "    - [Submission Preparation](###Submission-Preparation)\n",
    "    - [Submission to Kaggle](###Submission-to-Kaggle)\n",
    "8. [Conclusion](##Conclusion)\n",
    "    - [Summary](##Summary)\n",
    "    - [Future Work](###Future-Work)\n",
    "9. [References](##References)\n",
    "10. [Appendices](##Appendices)\n",
    "    - [Supplementary Scripts](###Supplementary-Scripts)\n",
    "    - [Additional Resources](###Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "A dataset containing information of accommodations published in AirBnB with their respective prices is presented. The size of the train dataset is approximately 1.5 Gb, and 0.5 Gb for the test dataset. This has 84 predictor variables that can be used as they see fit.\n",
    "\n",
    "The objective is to assign the correct price to the listed accommodations. \n",
    "\n",
    "In addition to the dataset, you are provided with this notebook containing the data loading script and a baseline model corresponding to a feed forward architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The primary objective of this project is to develop a predictive model capable of accurately estimating the rental prices of accommodations listed on AirBnB. By leveraging a dataset consisting of various attributes and historical pricing data of listed accommodations, we aim to build a model that minimizes the error in price prediction. \n",
    "\n",
    "The success of this endeavor will be evaluated based on the Mean Absolute Error (MAE) metric, with the goal of achieving an MAE of less than 70 points (based on the [participation in Kaggle](##Participation-in-Kaggle-Competition)). This objective aligns with the criteria set forth in the associated Kaggle competition, which serves as a structured platform for benchmarking the performance of our model against others.\n",
    "\n",
    "Several tasks have been outlined to aid in the accomplishment of this objective:\n",
    "\n",
    "- Conduct thorough [exploratory data analysis](###Exploratory-Data-Analysis-(EDA)) to understand the underlying patterns and characteristics of the data.\n",
    "- [Preprocess the data](###Data-Preprocessing) to ensure it is well-suited for training machine learning models.\n",
    "- Establish a [baseline model](##Baseline-Model) using a feed-forward neural network architecture, against which further models and techniques can be compared.\n",
    "- Engage in a methodical [grid search](###Grid-Search-Setup) to identify the optimal hyperparameters for our model, utilizing tools such as [Weights and Biases](https://wandb.ai/site) for systematic exploration and logging.\n",
    "- Incorporate advanced techniques including [Batch Normalization](###Batch-Normalization) and [Gradient Normalization/Clipping](###Gradient-Normalization-and/or-Gradient-Clipping) to enhance the learning process and stability of the model.\n",
    "- Continuously evaluate the performance of the model, iterating on the architecture and training process as necessary to inch closer to the desired MAE goal.\n",
    "\n",
    "By adhering to a structured approach encompassing data exploration, preprocessing, model building, hyperparameter tuning, and advanced technique implementation, we aspire to develop a robust model that stands up to the competition standards and possibly exceeds them, thereby moving closer to solving the real-world problem of accurate price prediction in the peer-to-peer accommodation rental domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "\n",
    "The dataset provided for this project comprises information pertaining to accommodations listed on AirBnB, captured across 85 different attributes or columns. This dataset is housed within a Pandas DataFrame and totals 326,287 entries, extending from index 0 to 326,286. Below is a high-level summary of the dataset's structure and contained attributes:\n",
    "\n",
    "- **Entries:** 326,287\n",
    "- **Attributes:** 85\n",
    "- **Target Variable:** `Price`\n",
    "- **Data Types:** \n",
    "    - Integer: 2\n",
    "    - Float: 31\n",
    "    - Object: 52\n",
    "- **Memory Usage:** 211.6+ MB\n",
    "\n",
    "#### Attribute Highlights\n",
    "\n",
    "1. **Identifier Attributes:**\n",
    "   - `id`: Unique identifier for each listing.\n",
    "   - `Host ID`: Unique identifier for each host.\n",
    "\n",
    "2. **Textual Descriptions:**\n",
    "   - `Name`, `Summary`, `Description`: Textual descriptions of the listing.\n",
    "   - `Neighborhood Overview`, `Notes`, `Transit`: Additional textual information about the listing’s neighborhood and transit options.\n",
    "\n",
    "3. **Host Information:**\n",
    "   - `Host Name`, `Host Since`, `Host Location`: Information regarding the host.\n",
    "   - `Host Response Time`, `Host Response Rate`: Host’s responsiveness metrics.\n",
    "\n",
    "4. **Location and Property Attributes:**\n",
    "   - `Street`, `Neighbourhood`, `City`, `State`, `Country`: Location-related attributes.\n",
    "   - `Property Type`, `Room Type`: Descriptors of the property type and room type.\n",
    "\n",
    "5. **Accommodation Features:**\n",
    "   - `Accommodates`, `Bathrooms`, `Bedrooms`, `Beds`: Attributes indicating the accommodation capacity and facilities.\n",
    "   - `Amenities`: List of amenities provided.\n",
    "\n",
    "6. **Pricing and Booking Information:**\n",
    "   - `Price`, `Security Deposit`, `Cleaning Fee`: Pricing-related information.\n",
    "   - `Guests Included`, `Extra People`, `Minimum Nights`, `Maximum Nights`: Booking-related attributes.\n",
    "\n",
    "7. **Availability and Review Metrics:**\n",
    "   - `Availability 30`, `Availability 60`, `Availability 90`, `Availability 365`: Availability metrics over different time horizons.\n",
    "   - `Number of Reviews`, `Review Scores Rating`, `Reviews per Month`: Review-related metrics.\n",
    "\n",
    "8. **Miscellaneous:**\n",
    "   - `Features`: Other features of the listing.\n",
    "   - `Geolocation`: Geographical coordinates of the listing.\n",
    "\n",
    "This dataset presents a rich and diverse set of attributes, offering a substantial foundation upon which to build predictive models aimed at accurately estimating rental prices for AirBnB listings. The extensive variety of data attributes spans textual descriptions, categorical variables, numerical metrics, and date-related information, providing a well-rounded basis for a comprehensive exploratory data analysis (EDA) and subsequent model development.\n",
    "\n",
    "> The memory usage of this dataset is significant, amounting to over 211.6 MB, which necessitates efficient data handling and processing techniques to ensure smooth and effective model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "In this project, we will be leveraging the powerful capabilities of [Keras](https://keras.io/) to build and train our machine learning models. Keras is an open-source software library that provides a Python interface for artificial neural networks. It acts as an interface for the TensorFlow library, allowing for high-level building and training of models.\n",
    "\n",
    "To ensure that the necessary dependencies are correctly installed and managed throughout the project, we'll be utilizing [Conda](https://docs.conda.io/en/latest/) as our package manager. Conda is an open-source package management and environment management system that runs on Windows, macOS, and Linux.\n",
    "\n",
    "The `environment.yml` file located in the root of the project directory contains the list of all necessary packages and their respective versions required for this project. This file will allow us to create a Conda environment with the specified dependencies, ensuring a consistent environment across different setups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch is an open-source machine learning library used for a variety of tasks,\n",
    "# but primarily for training deep neural networks.\n",
    "import torch\n",
    "\n",
    "# nn is a sub-module in PyTorch that contains useful classes and functions to build neural networks.\n",
    "import torch.nn as nn\n",
    "\n",
    "# F is a sub-module in PyTorch that contains useful functions for building neural networks.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DataLoader is a PyTorch utility for loading and batching data efficiently.\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# torchvision contains various utilities, pre-trained models, and datasets specifically\n",
    "# geared towards computer vision tasks.\n",
    "import torchvision\n",
    "\n",
    "# transforms are a set of common image transformations that are often required when\n",
    "# working with image data.\n",
    "from torchvision import transforms\n",
    "\n",
    "# ImageFolder is a utility for loading images directly from a directory structure where\n",
    "# each sub-directory represents a different class.\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# random_split is a utility function to randomly split a dataset into non-overlapping\n",
    "# new datasets of given lengths.\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "# SummaryWriter is a PyTorch utility for logging information to be displayed in TensorBoard.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# summary is a PyTorch utility for displaying the summary of a PyTorch model.\n",
    "from torchinfo import summary\n",
    "\n",
    "# tqdm is a Python library that adds a progress bar to an iterable object.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Matplotlib is a plotting library that is useful for visualizing data, plotting graphs, etc.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn is a Python data visualization library based on Matplotlib.\n",
    "import seaborn as sns\n",
    "\n",
    "# NumPy is a library for numerical operations and is especially useful for array and\n",
    "# matrix computations.\n",
    "import numpy as np\n",
    "\n",
    "# Pandas is a library for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# PIL is a library for image processing.\n",
    "from PIL import Image\n",
    "\n",
    "# os is a Python module that provides a portable way of using operating system dependent\n",
    "import os\n",
    "\n",
    "# time is a module that provides various time-related functions.\n",
    "import time\n",
    "\n",
    "# random is a module that implements pseudo-random number generators for various distributions.\n",
    "import random\n",
    "\n",
    "# accuracy_score computes the accuracy classification score.\n",
    "# confusion_matrix computes confusion matrix to evaluate the accuracy of a classification.\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# itertools is a module that provides various functions that work on iterators to produce\n",
    "from itertools import product\n",
    "\n",
    "# math is a module that provides access to the mathematical functions.\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Random Seed for Reproducibility\n",
    "\n",
    "For any machine learning experiment, reproducibility is crucial. Setting a random seed ensures that the random numbers generated by our code are the same across different runs, making the results reproducible. In this project, the random seed is set for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 117\n",
    "# Set the seed for generating random numbers\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "In this section, we delve into the initial phase of our project where we load the dataset into our environment and perform an exploratory data analysis (EDA) to better understand the nature and characteristics of the data we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "The dataset for this project is conveniently provided and is located at the `/data` directory. Since this dataset it's too large to be uploaded to GitHub, it is not included in the repository. However, it can be downloaded from the [Kaggle competition page](https://www.kaggle.com/competitions/obligatorio-deep-learning-2023).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data/public_train_data.csv'\n",
    "SUBMISSION_PATH = './data/private_data_to_predict.csv'\n",
    "train_df = pd.read_csv(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "The Exploratory Data Analysis (EDA) is a vital step that helps in understanding the intricacies of the data, spotting any anomalies, and uncovering patterns that could be instrumental in building a precise predictive model. The steps involved in the EDA for this dataset are outlined as follows:\n",
    "\n",
    "1. **Summary Statistics**\n",
    "   Acquiring summary statistics will provide insights into the central tendency and dispersion of the numerical attributes.\n",
    "\n",
    "2. **Data Type Analysis**\n",
    "   A review of the data types of each attribute to ensure they are in the correct format for analysis and modeling.\n",
    "\n",
    "3. **Missing Values Assessment**\n",
    "   Identifying and addressing missing values across different attributes to ensure completeness of the data.\n",
    "\n",
    "4. **Categorical Variable Analysis**\n",
    "   Exploring the unique values and counts of categorical variables to understand the distribution across different categories.\n",
    "\n",
    "5. **Correlation Analysis**\n",
    "   Analyzing the correlation between numerical variables, especially with respect to the target variable `Price`, to understand any strong relationships that might exist.\n",
    "\n",
    "6. **Visualization**\n",
    "   Employing visualization techniques to create histograms, box plots, and scatter plots to visualize data distribution, outliers, and relationships between variables.\n",
    "\n",
    "7. **Text Data Overview**\n",
    "   Reviewing textual data to understand the quality and potential feature extraction opportunities it presents.\n",
    "\n",
    "Through a detailed EDA, the aim is to garner insights that will be pivotal in guiding the subsequent data preprocessing and model building stages, thereby ensuring a solid foundation for developing an accurate price prediction model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics\n",
    "\n",
    "Summary statistics provide a high-level overview of the numerical attributes within the dataset, offering insights into the central tendency, dispersion, and shape of the distribution of the dataset, sans any influence of the other attributes. These statistics are crucial for understanding the typical behavior of the dataset, identifying outliers, and observing the distribution and spread of the data points across different attributes.\n",
    "\n",
    "##### Key Components of Summary Statistics:\n",
    "\n",
    "1. **Count:** The number of non-null entries for each attribute.\n",
    "2. **Mean:** The average value of each attribute, providing a measure of central tendency.\n",
    "3. **Standard Deviation (std):** A measure of the amount of variation or dispersion of a set of values.\n",
    "4. **Minimum (min) and Maximum (max):** The smallest and largest values in each attribute, respectively.\n",
    "5. **25th, 50th (median), and 75th Percentiles:** These values provide a summary of the distribution of values, where for instance, 25% of the data points are below the 25th percentile.\n",
    "\n",
    "A tabulated summary of these statistics can be procured for each numerical attribute in the dataset. This tabulated format allows for a clear, concise view of the dataset's overall behavior, and aids in identifying any potential anomalies or outliers that may require further investigation.\n",
    "\n",
    "Furthermore, summary statistics play a vital role in the data preprocessing stage, where understanding the distribution of data is crucial for tasks such as normalization, handling outliers, and feature scaling. By thoroughly analyzing these summary statistics, one can make informed decisions on the necessary preprocessing steps to enhance the model's performance in subsequent stages of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics = train_df.describe()\n",
    "\n",
    "# Displaying the summary statistics\n",
    "print(summary_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Type Analysis\n",
    "\n",
    "Analyzing the data types of each attribute is a crucial step in understanding the kind of data you are dealing with. This analysis helps in ensuring that each attribute is formatted correctly, which is essential for both data preprocessing and modeling stages of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values Assessment\n",
    "\n",
    "Assessing missing values is a critical step in the data exploration process. Missing data can lead to incorrect or biased analyses and conclusions. Identifying the presence and extent of missing values in the dataset is crucial to decide on the appropriate handling strategies.\n",
    "\n",
    "Implementing appropriate strategies to handle missing values is crucial to ensure the robustness and accuracy of the predictive model. The strategy chosen can significantly affect the model's performance and the insights derived from the data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a heatmap to visualize the missing values\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(train_df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing values heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "df_numeric = train_df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "# Get correlation of all features with 'price'\n",
    "price_correlation = correlation_matrix['Price'].sort_values(ascending=False)\n",
    "\n",
    "# Filter out the features with a correlation above a certain threshold, for example 0.3\n",
    "important_features = price_correlation[abs(price_correlation) >= 0.1]\n",
    "\n",
    "# Room Type, Smart Location\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "## Baseline Model\n",
    "### Model Architecture\n",
    "### Model Compilation\n",
    "### Model Training\n",
    "### Model Evaluation\n",
    "## Hyperparameter Tuning\n",
    "### Grid Search Setup\n",
    "### Execution of Grid Search\n",
    "### Analysis of Grid Search Results\n",
    "## Advanced Techniques Implementation\n",
    "### Batch Normalization\n",
    "### Gradient Normalization and\n",
    "## Final Model\n",
    "### Model Architecture\n",
    "### Model Compilation\n",
    "### Model Training\n",
    "### Model Evaluation\n",
    "## Participation in Kaggle Competition\n",
    "### Submission Preparation\n",
    "### Submission to Kaggle\n",
    "## Conclusion\n",
    "## Summary\n",
    "### Future Work\n",
    "## References\n",
    "## Appendices\n",
    "### Supplementary Scripts\n",
    "### Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taller_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
